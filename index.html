---
layout: default
tags: about
---
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script type="text/javascript">
  function readMore() {
    $('#readMore').hide();
    $('#more').show();
  }
  function readLess() {
    $('#readMore').show();
    $('#more').hide();
  }
</script>
<img src="{{ site.baseurl }}/images/me.jpg" alt="Binhui Xie" width="160"
  style="float: right; padding: 10px 0 0 15px; border-radius: 0%;" />

<div class="bio" style="text-align:justify">
  <!-- <hr/> -->
  <p>
    Binhui Xie is a fifth year Ph.D. student in the <a href="https://cs.bit.edu.cn/"
    class="uline">School of Computer Science and Technology</a>
     at <a href="https://english.bit.edu.cn/"
     class="uline">Beijing Institute of Technology</a>, advised by <a href="https://shuangli.xyz"
    class="uline">Prof. Shuang Li</a> and Prof. Chi Liu. Before that, he received his B.E. degree in Software Engineering at <a href="https://english.bit.edu.cn/"
    class="uline">BIT</a> in 2019. 
    
    <p>
      My research interests are in transfer learning, multimodal models, and efficient fine-tuning.
    </p>
    
  <p>
    <b>Contact</b>: I'm always happy to discuss or collaborate! Feel free to seed me an email(<a
      href="mailto:binhuixie@bit.edu.cn" class="uline">binhuixie@bit.edu.cn</a>; <a
      href="mailto:minexbh@gmail.com" class="uline">minexbh@gmail.com</a>) if you're interested.
  </p>

  <p>
    <font color="red">I’m on job market now! If you are interested in me, contact me via Email.</font> (Download my <a href="https://binhuixie.github.io/cv/CV.pdf"
      class="uline">resumé</a>)
    
  </p>

  <br />

  <!-- <div class="container">
    <div class="row" style="text-align:center;">
      <div class="col-4">
        <div class="row">
          <div class="col-6">
            <a href="http://www.bits-pilani.ac.in/"><img src="images/bits.png" style="max-height:150px;width:80%"></a>
          </div>
          <div class="col-6">
            <a href="https://www.gatech.edu/"><img src="images/gt.jpg" style="max-height:150px;width:80%"></a>
          </div>
        </div>
      </div>
      <div class="col-8">
        <div class="row" style="text-align:center;">
          <div class="col-3">
            <a href="https://www.adobe.com/in/"><img src="images/adobe.png" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="http://curai.com/"><img src="images/curai.png" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="https://einstein.ai/"><img src="images/sf.png" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="https://nv-tlabs.github.io/"><img src="images/nvidia.png" style="width:80%;max-height:150px"></a>
          </div>
        </div>
      </div>
    </div>
    <div class="row" style="text-align:center;">
      <div class="col-4">
        <div class="row">
          <div class="col-6">
            <div style="padding:10px">
              <h6>Fall '11-Spring '15</h6>
            </div>
          </div>
          <div class="col-6">
            <div style="padding:10px">
              <h6>Fall '17-current</h6>
            </div>
          </div>
        </div>
      </div>
      <div class="col-8">
        <div class="row" style="text-align:center;">
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '14, Fall '15- Summer '16</h6>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '18, '19</h6>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '21</h6>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <h6>Summer '22</h6>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div> -->

</div>


<hr />
<div class="news">
  <h2>News</h2>
  <br />
  <ul>

    

    <!-- <li>
      <p>
        [May 2024] Our PaRe on cross-modal fine-tuning is accepted by ICML 2024, congrats!
      </p>
    </li> -->

    <li>
      <p>
        [Nov 2023] SePiCo is selected as <font color="red">Highly Cited Paper</font>!! [<a href="images/highly-sepico.png" class="uline">screenshot</a>]
      </p>
    </li>

    <li>
      <p>
        [Nov 2023] CAF is selected as <font color="red">Highly Cited Paper</font>! [<a href="images/highly-caf.png" class="uline">screenshot</a>]
      </p>
    </li>

    <li>
      <p>
        [Sep 2023] EVA is selected as the 7th <font color="red">most influential paper</font> in CVPR 2023! 
        [<a  href="https://www.paperdigest.org/2023/09/most-influential-cvpr-papers-2023-09/" class="uline">list</a>]
      </p>
    </li>

    <li>
      <p>
        [Sep 2023] Annotator for label-efficient LiDAR segmentation is accepted by NeurIPS 2023! 
        [<a  href="https://arxiv.org/abs/2310.20293">pdf</a>] [<a href="https://github.com/BIT-DA/Annotator">code</a>]
      </p>
    </li>

    <li>
      <p>
        [Feb 2023] Our EVA (<font color="red">Highlight</font>) on vision foundation models and RoTTA on test-time adaptation are accepted by CVPR 2023, congrats!
      </p>
    </li>

    <li>
      <p>
        [Jan 2023] Our work on cross-domain semantic segmentation (SePiCo) is accepted by <font color="red">T-PAMI</font> (IF: 23.6)! 
        [<a href="https://arxiv.org/abs/2204.08808">pdf</a>] [<a href="https://github.com/BIT-DA/SePiCo">code</a>]
      </p>
    </li>

    <li>
      <p>
        [Nov 2022] Our study on adverse-condition semantic segmentation (VBLC) is accepted by AAAI 2023 as <font color="red">Oral</font>, congrats! 
        [<a href="https://arxiv.org/abs/2211.12256">pdf</a>] [<a href="https://github.com/BIT-DA/VBLC">code</a>]
      </p> 
    </li>

    <li>
      <p>
        [Nov 2022] EVA-01 Launched! 
        [<a href="https://arxiv.org/abs/2211.07636">pdf</a>] [<a href="https://github.com/baaivision/EVA">code</a>]
      </p>
    </li>

    <li>
      <p>
        [Jun 2022] CAF is accepted by TKDE!
        [<a href="https://ieeexplore.ieee.org/document/9803869">pdf</a>] [<a href="https://github.com/BIT-DA/CAF">code</a>]
      </p> 
    </li>

    <li>
      <p>
        [Mar 2022] Our work on data-efficient semantic segmentation (RIPU) is accepted by CVPR 2022 as <font color="red">Oral</font>!
        [<a href="https://arxiv.org/abs/2111.12940">pdf</a>] [<a href="https://github.com/BIT-DA/RIPU">code</a>]
      </p> 
    </li>

    <li>
      <p>
        [Dec 2021] Our work on active domain adaptation (EADA) is accepted by AAAI 2022!
        [<a href="https://arxiv.org/abs/2112.01406">pdf</a>] [<a href="https://github.com/BIT-DA/EADA">code</a>]
      </p> 
    </li>

    <li>
      <p>
        [Feb 2021] The extension of DCAN got accepted by <font color="red">T-PAMI</font> (IF: 23.6), congrats! 
        [<a href="https://arxiv.org/abs/2103.12339">pdf</a>] [<a href="https://github.com/BIT-DA/GDCAN">code</a>]
      </p>
    </li>

    <li>
      <p>
        [Jul 2020] Our work on heterogeneous domain adaptation (SSAN) is accepted by ACM MM 2020!
        [<a href="https://arxiv.org/abs/2008.01677">pdf</a>] [<a href="https://github.com/BIT-DA/SSAN">code</a>]
      </p>
    </li>

    <li>
      <p>
        [Jul 2019] My starting research journey on domain adaptation (JADA) is accepted by ACM MM 2019!
        [<a href="https://dl.acm.org/doi/abs/10.1145/3343031.3351070">pdf</a>] [<a href="https://github.com/BIT-DA/JADA">code</a>]
      </p>
    </li>

  </ul>
  <!-- <a id="readMore" class="uline" href="#" onclick="readMore();return false;">
    <div style='text-align: center;'>
      <h4>Read More</h4>
    </div>
  </a>
  <span id="more">
    <ul>
      <li>
        <p>[Apr 2022] Our <a href='https://ieeexplore.ieee.org/document/9756672' class='uline'>CSDN</a> for
          partial domain adaptation is accepted by T-CYB!</p>
      </li>
      <li>
        <p>[Dec 2020] Our <a href='https://arxiv.org/abs/2012.06995' class='uline'>BCDM</a> for bi-classifier
          adversarial adaptation is accepted by AAAI 2021!</p>
      </li>

      
      <li>
        <p> [Jan 2020] Our <a href="https://ieeexplore.ieee.org/document/8951259" class="uline">DTLC</a> for shallow
          domain adaptation is accepted by TNNLS! </p>
      </li>

      <li>
        <p>[Dec 2019] Our <a href='https://arxiv.org/abs/2005.06717' class='uline'>DCAN</a> for domain adaptation is
          accepted by AAAI 2020!</p>
      </li>
   
  </span>
  </ul>
  <a id="readLess" class="uline" href="#" onclick="readLess();return false;">
    <div style='text-align: center;'>
      <h4>Read Less</h4>
    </div>
  </a> -->
</div>

<hr />

<div id="research">
  <h2><a name="research">Selected Publications</a></h2>
  <br />
  <table width="100%" align="center" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>

      <!-- CVPR EVA -->
      <tr>
        <td valign="middle" width="35%">
          <div class="one" style="text-align:center;">
            <img src="images/cvpr2023_eva.png" style="max-height: 300px;">
          </div>
        </td>
        <td valign="middle" width="65%">
          <h5>
            EVA: Exploring the Limits of Masked Visual Representation Learning at Scale
          </h5>
          <p>
            IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b> <font color="red">Highlights</font>), 2023
          </p>
          <p>
            <a href="https://www.paperdigest.org/2023/09/most-influential-cvpr-papers-2023-09/" class="uline-special"><span style="color:red">CVPR 2023 Top-10 Influential Papers (Rank 7)</span></a>
          </p>
          <p>
            Yuxin Fang, Wen Wang, <b>Binhui Xie</b>, Quan Sun, Ledell Yu Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,
            and Yue Cao
          </p>
          <p>
            EVA is the first open-sourced billion-scale vision foundation model that achieves state-of-the-art
            performance on a broad range of downstream tasks.
          </p>
          <a href="https://arxiv.org/abs/2211.07636" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Paper</a>
          <a href="https://github.com/baaivision/EVA" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>
        </td>
      </tr>
    

    <!-- TPAMI SePiCO -->
    <tr>
      <td valign="middle" width="35%">
        <div class="one" style="text-align:center;">
          <img src="images/SePiCo.png" style="max-height: 300px;">
        </div>
      </td>
      <td valign="middle" width="65%">
        <h5>
          <i class="fa fa-trophy" aria-hidden="true" style="color: red;"></i> SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation
        </h5>
        <p>
          IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>, IF: 23.6), 2023
        </p>
        <p>
          <a href="images/highly-sepico.png" class="uline-special"><span style="color:red">ESI Highly Cited Paper</span></a>
        </p>

        <p>
          <b>Binhui Xie</b>, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang
        </p>
        <p>
          A novel one-stage adaptation framework that highlights the semantic concepts of individual pixel to promote
          learning of class-discriminative and class-balanced pixel embedding space across domains.
        </p>
        <a href="https://binhuixie.github.io/sepico-web" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Project Page</a>
        <a href="https://arxiv.org/abs/2204.08808" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Paper</a>
        <a href="https://github.com/BIT-DA/SePiCo" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Code</a>
      </td>
    </tr>

    <!-- NeurIPS Annotator -->
    <tr>
      <td valign="middle" width="35%">
        <div class="one" style="text-align:center;">
          <img src="images/neurips2023_annotator.png" style="max-height: 300px;">
        </div>
      </td>
      <td valign="middle" width="65%">
        <h5>
          Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation
        </h5>
        <p>
          Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2023
        </p>
        <p>
          <b>Binhui Xie</b>, Shuang Li, Qingju Guo, Chi Harold Liu, and Xinjing Cheng
        </p>
        <p>
          We established a simple and general baseline for label-efficient LiDAR semantic segmentation.
        </p>
        <a href="https://binhuixie.github.io/annotator-web" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Project Page</a>
        <a href="https://arxiv.org/abs/2310.20293" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Paper</a>
        <a href="https://github.com/BIT-DA/Annotator" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Code</a>
        <a href="https://www.bilibili.com/video/BV1oN41137NH"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Video</a>
          <a href="https://www.dropbox.com/scl/fi/8ca79302y9168iu0ctxyv/virtual-video-5min.pdf?rlkey=gkc90hg35h1hfsoyol4clmzby&dl=0"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Slides</a>
        <a href="https://www.dropbox.com/scl/fi/esor1vqrtklumbtb5l5j8/annotator_poster.pdf?rlkey=vi3tibnzowuzo4qmfy2r33vjh&dl=0"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
      </td>
    </tr>

    <!-- CVPR RIPU -->
    <tr>
      <td valign="middle" width="35%">
        <div class="one" style="text-align:center;">
          <img src="images/cvpr2022_RIPU.png" style="max-height: 300px;">
        </div>
      </td>
      <td valign="middle" width="65%">
        <h5>
          Towards Fewer Annotations: Active Learning via Region Impurity and Prediction Uncertainty for Domain
          Adaptive Semantic Segmentation
        </h5>
        <p>
          IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b> <font color="red">Oral</font>), 2022
        </p>
        <p>
          <b>Binhui Xie</b>, Longhui Yuan, Shuang Li, Chi Harold Liu, and Xinjing Cheng
        </p>
        <p>
          A region-based acquisition strategy for DA Segmentation queries image regions that are both diverse in
          spatial adjacency and uncertain in prediction output.
        </p>
        <a href="https://arxiv.org/abs/2111.12940" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Paper</a>
        <a href="https://github.com/BIT-DA/RIPU" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Code</a>
        <a href="https://www.bilibili.com/video/BV1oS4y1e7J5?spm_id_from=333.1007.top_right_bar_window_default_collection.content.click&vd_source=2536293932098e7a347341a231b3fb8b"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Video</a>
        <a href="https://www.dropbox.com/s/a8ty5l2sut89b6l/5%20min_pre.pdf?dl=0"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Slides</a>
        <a href="https://www.dropbox.com/s/mm14k36ydirk2w8/cvpr22_poster_2x1_in-person.pdf?dl=0"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
      </td>
    </tr>

    <!-- AAAI EADA -->
    <tr>
      <td valign="middle" width="35%">
        <div class="one" style="text-align:center;">
          <img src="images/aaai2022_EADA.png" style="max-height: 300px;">
        </div>
      </td>
      <td valign="middle" width="65%">
        <h5>
          Active Learning for Domain Adaptation: An Energy-based Approach
        </h5>
        <p>
          AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2022
        </p>
        <p>
          <b>Binhui Xie</b>, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang
        </p>
        <p>
          A new perspective to select a highly informative subset of unlabeled target data under domain shift via
          exploiting free energy biases across domains.
        </p>
        <a href="https://arxiv.org/abs/2112.01406" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Paper</a>
        <a href="https://github.com/BIT-DA/EADA" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Code</a>
        <a href="https://www.bilibili.com/video/BV1qa411h7Xm?share_source=copy_web"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Video</a>
        <a href="https://www.dropbox.com/s/8ozwc8uw1q1tqlf/eada_slides.pdf?dl=0"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Slides</a>
        <a href="https://www.dropbox.com/s/xdtcd3iifb7xd06/eada_poster.pdf?dl=0"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
      </td>
    </tr>

    <!-- TKDE CAF -->
    <tr>
      <td valign="middle" width="35%">
        <div class="one" style="text-align:center;">
          <img src="images/tkde_CAF.png" style="max-height: 300px;">
        </div>
      </td>
      <td valign="middle" width="65%">
        <h5>
          <i class="fa fa-trophy" aria-hidden="true" style="color: red;"></i> A Collaborative Alignment Framework of Transferable Knowledge Extraction for Unsupervised Domain Adaptation
        </h5>
        <p>
          IEEE Transactions on Knowledge and Data Engineering (<b>TKDE</b>), 2023
        </p>

        <p>
          <a href="images/highly-caf.png" class="uline-special"><span style="color:red">ESI Highly Cited Paper</span></a>
        </p>

        <p>
          <b>Binhui Xie</b>, Shuang Li, Fangrui Lv, Chi Harold Liu, Guoren Wang, and Dapeng Wu
        </p>
        <p>
          We introduce a collaborative alignment framework that integrates global structure information and local
          semantic consistency into a unified deep model.
        </p>
        <a href="https://ieeexplore.ieee.org/document/9803869" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Paper</a>
        <a href="https://github.com/BIT-DA/CAF" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Code</a>
      </td>
    </tr>

    <!-- TPAMI GDCAN -->
    <tr>
      <td valign="middle" width="35%">
        <div class="one" style="text-align:center;">
          <img src="images/tpami_GDCAN.png" style="max-height: 300px;">
        </div>
      </td>
      <td valign="middle" width="65%">
        <h5>
          Generalized Domain Conditioned Adaptation Network
        </h5>
        <p>
          IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>, IF: 23.6), 2022
        </p>
        <p>
          Shuang Li, <b>Binhui Xie</b>, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang
        </p>
        <p>
          We develop GDCAN to automatically determine whether domain channel activations should be separately modeled
          in each attention module for domain adaptaion problem.
        </p>
        <a href="https://arxiv.org/abs/2103.12339" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Paper</a>
        <a href="https://github.com/BIT-DA/GDCAN" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Code</a>
        <a href="https://www.dropbox.com/s/9h6cbyl3x8lz7bw/gdcan_poster.pdf?dl=0"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
      </td>
    </tr>

    <!-- CVPR RoTTA -->
    <tr>
      <td valign="middle" width="35%">
        <div class="one" style="text-align:center;">
          <img src="images/cvpr2023_rotta.png" style="max-height: 300px;">
        </div>
      </td>
      <td valign="middle" width="65%">
        <h5>
          Robust Test-Time Adaptation in Dynamic Scenarios
        </h5>
        <p>
          IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023
        </p>
        <p>
          Longhui Yuan, <b>Binhui Xie</b> and Shuang Li
        </p>
        <p>
          RoTTA presents a new practical test-time adaptation setting where environments gradually change and the test
          data is sampled correlatively over time.
        </p>
        <a href="https://arxiv.org/abs/2303.13899" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Paper</a>
        <a href="https://github.com/BIT-DA/RoTTA" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Code</a>
        <a href="https://www.youtube.com/watch?v=dEhpVOM0I6Q" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Video</a>
      </td>
    </tr>

    <!-- AAAI VBLC -->
    <tr>
    <td valign="middle" width="35%">
      <div class="one" style="text-align:center;">
        <img src="images/aaai2023_VBLLC.png" style="max-height: 300px;">
      </div>
    </td>
    <td valign="middle" width="65%">
      <h5>
        VBLC: Visibility Boosting and Logit-Constraint Learning for Domain Adaptive Semantic Segmentation under
        Adverse Conditions
      </h5>
      <p>
        AAAI Conference on Artificial Intelligence (<b>AAAI</b> <font color="red">Oral</font>), 2023
      </p>
      <p>
        Mingjia Li*, <b>Binhui Xie*</b>, Shuang Li, Chi Harold Liu, and Xinjing Cheng
      </p>
      <p>
        VBLC tackles the problem of domain adaptive semantic segmentation under adverse conditions, getting rid of
        reference images.
      </p>
      <a href="https://kiwixr.github.io/projects/vblc" class="btn btn-outline-primary btn-sm" role="button"
      aria-pressed="true">Project Page</a>
      <a href="https://arxiv.org/abs/2211.12256" class="btn btn-outline-primary btn-sm" role="button"
        aria-pressed="true">Paper</a>
      <a href="https://github.com/BIT-DA/VBLC" class="btn btn-outline-primary btn-sm" role="button"
        aria-pressed="true">Code</a>
      <a href="https://www.youtube.com/watch?v=o0voyaQ3FzM" class="btn btn-outline-primary btn-sm" role="button"
        aria-pressed="true">Video</a>
      <a href="https://www.dropbox.com/s/pvb2701k2gr9cfb/aaai23poster.pdf?dl=0"
        class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
    </td>
  </tr>

    <!-- MM SSAN -->
    <tr>
      <td valign="middle" width="35%">
        <div class="one" style="text-align:center;">
          <img src="images/mm2020_SSAN.png" style="max-height: 300px;">
        </div>
      </td>
      <td valign="middle" width="65%">
        <h5>
          Simultaneous Semantic Alignment Network for Heterogeneous Domain Adaptation
        </h5>
        <p>
          ACM International Conference on Multimedia (<b>ACM MM</b>), 2020
        </p>
        <p>
          Shuang Li, <b>Binhui Xie</b>, Jiashu Wu, Ying Zhao, Chi Harold Liu, and Zhengming Ding
        </p>
        <p>
          A HDA method simultaneously exploits correlations among categories and aligns the centroids for each
          category across domains.
        </p>
        <a href="https://arxiv.org/abs/2008.01677" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Paper</a>
        <a href="https://github.com/BIT-DA/SSAN" class="btn btn-outline-primary btn-sm" role="button"
          aria-pressed="true">Code</a>
        <a href="https://www.dropbox.com/s/k7lu7zvkfzlv2nj/ssan_slides.pdf?dl=0"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Slides</a>
        <a href="https://www.dropbox.com/s/jjy5itt5niqp8b0/ssan_poster.pdf?dl=0"
          class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
      </td>
    </tr>

      <!-- AAAI BCDM -->
      <!-- <tr>
              <td valign="middle" width="35%">
                  <div class="one" style="text-align:center;">
                      <img src="images/aaai2021_BCDM.png" style="max-height: 300px;">
                  </div>
              </td>
              <td valign="middle" width="65%">
                  <h5>
                      Bi-classifier determinacy maximization for unsupervised domain adaptation
                  </h5>
                  <p>
                      <a href="https://aaai.org/Conferences/AAAI-22/" class="uline-special"><span style="color:red">AAAI 2021</span></a>
                  </p>
                  <p>
                      Shuang Li, Fangrui Lv, <b>Binhui Xie</b>, Chi Harold Liu, Jian Liang, Chen Qin
                  </p>
                  <p>
                      We design a novel CDD metric, which formulates classifier discrepancy as the class relevance of distinct target predictions and implicitly introduces constraint on the target feature discriminability.
                  </p>
                  <a href="https://arxiv.org/abs/2012.06995" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                  <a href="https://github.com/BIT-DA/BCDM" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
              </td>
          </tr> -->


      <!-- MM JADA -->
      <!-- <tr>
              <td valign="middle" width="35%">
                  <div class="one" style="text-align:center;">
                      <img src="images/mm2019_JADA.png" style="max-height: 300px;">
                  </div>
              </td>
              <td valign="middle" width="65%">
                  <h5>
                      Joint Adversarial Domain Adaptation
                  </h5>
                  <p>
                      <a href="https://2019.acmmm.org" class="uline-special"><span style="color:red">MM 2019</span></a>
                  </p>
                  <p>
                      Shuang Li, Chi Harold Liu, <b>Binhui Xie</b>, Limin Su, Zhengming Ding, and Gao Huang
                  </p>
                  <p>
                      JADA simultaneously aligns domain-wise and class-wise distributions across source and target in a unified adversarial learning process.
                  </p>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3343031.3351070" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                  <a href="https://github.com/BIT-DA/JADA" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
                  <a href="https://www.dropbox.com/s/t6kxl5se1t7rla7/jada_poster.pdf?dl=0" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Poster</a>
              </td>
          </tr> -->


      <!-- TCYB CSDN -->
      <!-- <tr>
            <td valign="middle" width="35%">
                <div class="one" style="text-align:center;">
                    <img src="images/tcyb_CSDN.png" style="max-height: 300px;">
                </div>
            </td>
            <td valign="middle" width="65%">
                <h5>
                  Critical Classes and Samples Discovering for Partial Domain Adaptation
                </h5>
                <p>
                    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036" class="uline-special"><span style="color:red">T-CYB</span></a>
                </p>
                <p>
                  Shuang Li, Kaixiong Gong, <b>Binhui Xie</b>, Chi Harold Liu, Weipeng Cao, Song Tian
                </p>
                <p>
                    CSDN, which aims to solve PDA problem, identifies the most relevant source classes and critical target samples, such that more precise cross-domain alignment in the shared label space could be enforced by co-training two diverse classifiers. 
                </p>
                <a href="https://ieeexplore.ieee.org/document/9756672" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                <a href="https://github.com/BIT-DA/CSDN" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
            </td>
        </tr> -->


      <!-- AAAI DCAN -->
      <!-- <tr>
            <td valign="middle" width="35%">
                <div class="one" style="text-align:center;">
                    <div class="two" id="jump_image" style="opacity: 0;"></div> 
                    <img src="images/aaai2020_DCAN.png" style="max-height: 300px;">
                </div>
            </td>
            <td valign="middle" width="65%">
                <h5>
                    Domain Conditioned Adaptation Network
                </h5>
                <p>
                    <a href="https://aaai.org/Conferences/AAAI-22/" class="uline-special"><span style="color:red">AAAI 2020</span></a>
                </p>
                <p>
                    Shuang Li, Chi Harold Liu, Qiuxia Lin, <b>Binhui Xie</b>, Zhengming Ding, Gao Huang, Jian Tang
                </p>
                <p>
                    DCAN relaxes the shared-convnets assumption and excites distinct convolutional channels with a domain conditioned channel attention mechanism.
                </p>
                <a href="https://arxiv.org/abs/2005.06717" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
                <a href="https://github.com/BIT-DA/DCAN" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
            </td>
        </tr> -->


      <!-- TNNLS DTLC -->
      <!-- <tr>
          <td valign="middle" width="35%">
              <div class="one" style="text-align:center;">
                  <img src="images/tnnls_DTLC.png" style="max-height: 300px;">
              </div>
          </td>
          <td valign="middle" width="65%">
              <h5>
                Discriminative transfer feature and label consistency for cross-domain image classification
              </h5>
              <p>
                  <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" class="uline-special"><span style="color:red">T-NNLS</span></a>
              </p>
              <p>
                Shuang Li, Chi Harold Liu, Limin Su, <b>Binhui Xie</b>, Zhengming Ding, CL Philip Chen, Dapeng Wu
              </p>
              <p>
                DTLC can naturally unify cross-domain alignment with discriminative information preserved and label consistency of source and target data into one framework.
              </p>
              <a href="https://ieeexplore.ieee.org/abstract/document/8951259" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Paper</a>
              <a href="https://github.com/hnjzlishuang/DTLC" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
          </td>
      </tr> -->

    </tbody>
  </table>
</div>

<hr>

<!-- <div id="projects">
  <h2>Projects</h2>
  <table width="100%" align="center" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
      <tr>
        <td valign="top" width="50%">
          <h5>
            Fabrik: An Online Collaborative Neural Network Editor
          </h5>

          <p class="authors">
            Utsav Garg, <b>Viraj Prabhu</b>, Deshraj Yadav, Ram Ramrakhya, Harsh Agrawal, Dhruv Batra
          </p>
          <p class="authors">Lead mentor on Fabrik, an open-source web platform to collaboratively build, visualize, and
            design neural networks in the browser.</p>
          <a href="https://arxiv.org/abs/1810.11649" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Report</a>
          <a href="https://github.com/Cloud-CV/Fabrik" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>

        </td>
        <td width="50%">
          <div class="one" style="text-align:center;">
            <div class="two" id="jump_image" style="opacity: 0;"></div>
            <img src="images/fabrik.png" style="max-height: 150px;width:100%">
          </div>
        </td>
      </tr>

      <tr>
        <td valign="top" width="40%">
          <h5>PyTorch implementation of <a href="https://arxiv.org/abs/1703.06585" class="uline">Learning Cooperative
              Visual Dialog Agents with Deep Reinforcement Learning</a></h5>
          <p class="authors">
            Nirbhay Modhe, <b>Viraj Prabhu</b>, Michael Cogswell, Satwik Kottur, Abhishek Das, Stefan Lee, Devi Parikh,
            Dhruv Batra
          </p>
          <a href="https://github.com/batra-mlp-lab/visdial-rl" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Code</a>

        </td>
        <td width="50%">
          <div class="one" style="text-align:center;">
            <div class="two" id="jump_image" style="opacity: 0;"></div>
            <img src="images/visdial_rl.jpg" style="margin-top:30px;max-height: 200px;width:100%;">
          </div>
        </td>
      </tr>
      <td valign="top" width="75%">
        <h5>Adobe Captivate Prime</h5>
        <p class="authors">During my time as a software developer at Adobe (Aug '15-'16), I was responsible for the <a
            href="http://www.adobe.com/products/captivateprime.html" class="uline">Captivate Prime</a> Android app
          through two release cycles. I developed features for offline content play-back, syncing, and UI.
        </p>
      </td>
      <td width="25%">
        <div class="one" style="text-align:center;">
          <div class="two" style="opacity: 0;"></div>
          <img src="images/cp.jpeg" style="width:50%;">
        </div>
      </td>
      </tr>
      <tr>
        <td valign="top" width="75%">
          <h5>Automated camera calibration and boresighting</h5>
          <p class="authors">Over a research internship at <a href="http://tonboimaging.com/" class="uline">Tonbo
              Imaging</a> (Spring '15), I designed an algorithm for automated camera calibration and implemented a
            boresighting algorithm for the company's video precision boresight tool.
          </p>
          </p>
        </td>
        <td width="25%">
          <div class="one" style="text-align:center;">
            <div class="two" style="opacity: 0;"></div>
            <img src="images/tonbo.png" style="width:50%;">
          </div>
        </td>
      </tr>
      <tr>
        <td valign="top" width="75%">
          <h5>KeyframeCut</h5>
          <p class="authors">Over an internship at Adobe, I co-developed KeyframeCut, a fast graphcut-based segmentation
            algorithm for real-time background substitution in video. Transferred into Magic Green Screen, the marquee
            feature of Adobe Presenter Video Express 11.</p>
          <a href="https://www.youtube.com/watch?v=wM9mpFxaOpM" class="btn btn-outline-primary btn-sm" role="button"
            aria-pressed="true">Demo</a>
          <a href="http://blogs.adobe.com/captivate/2016/03/adobe-presenter-video-express-the-end-of-green-screen.html"
            class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Blog</a>
          </p>
        </td>
        <td width="25%">
          <div class="one" style="text-align:center;">
            <div class="two" style="opacity: 0;"></div>
            <img src="images/mgs.png" style="width:75%;">
          </div>
        </td>
      </tr>
    </tbody>
  </table>
</div> -->

<div id="talks">
  <h2>Invited Talks</h2>
  <br />
  <ul>
    <li>
      <p>
        2023.11, AI TIME (<a href="https://www.bilibili.com/video/BV1oN41137NH/" class="uline">online</a>), A Generic Active Learning
        Baseline in LiDAR Semantic Segmentation.
      </p>
    </li>
    <li>
      <p>
        2023.4, VALSE_Webinar (<a href="https://www.bilibili.com/video/BV1Xo4y1j7KR/"
          class="uline">recorded</a>), Towards Fewer Annotations: Active Learning for Adaptive Semantic Segmentation.
      </p>
    </li>
    <li>
      <p>
        2022.6, Synced (<a href="https://www.bilibili.com/video/BV19a411p7tg/"
          class="uline">online</a>), Towards Fewer Annotations for Adaptive Semantic Segmentation.
      </p>
    </li>
    <li>
      <p>
        2022.6, ReadPaper (<a
          href="https://www.bilibili.com/video/BV1oS4y1e7J5/"
          class="uline">online</a>), Active Learning for Adaptive Semantic Segmentation.
      </p>
    </li>
    <li>
      <p>
        2022.5, Zhidx Course (<a href="https://course.zhidx.com/c/Njg5MGIyMDlkYjhkZmVhMmI5ZTM="
          class="uline">onine</a>), Domain Adaptation meets Active Learning.
      </p>
    </li>
    <li>
      <p>
        2022.3, AI Drive (<a
          href="https://www.bilibili.com/video/BV1qa411h7Xm/"
          class="uline">online</a>), Energy-based Active Domain Adaptation.
      </p>
    </li>
    <li>
      <p>
        2021.10, VALSE (Hangzhou), spotlight & poster, Generalized Domain Conditioned Adaptation.
      </p>
    </li>
  </ul>

</div>

<hr>
<div class="news">
  <h2>Academic Service</h2>
  <ul>
    <li>
      <p>
        Conference Reviewer: ICML24, ICLR24/23, NeurIPS23 (<font color="red">Top Reviewers</font>@2023), CVPR24/23/22/21, ICCV23/21, ECCV24/22
      </p>
    </li>

    <li>
      <p>
        Journal Reviewer: IJCV, T-IP, T-KDE, T-CSVT, Pattern Recognition, T-IV
      </p>
    </li>

    <li>
      <p>
        Organizer of Workshop: VALSE 2022 (Student Workshop).
      </p>
    </li>
  </ul>
</div>


<hr>
<div class="news">
  <h2>Teaching Assistant</h2>
  <br />
  <p>
    Machine Learning Essentials, Spring 2021 & Spring 2022, Instructor: <a href="https://shuangli.xyz"
      class="uline">Prof. Li</a>
  </p>

</div>


<hr>
<div id="news">
  <h2>Selected Honors and Awards</h2>
  <ul>
    <li>
      <p>
        Scholar Award and Top Reviewers, NeurIPS 2023.
      </p>
    </li>
    
    <li>
      <p>
        Young Scientist Group (青源会), BAAI, 2023.
      </p>
    </li>

    <li>
      <p>
        National Scholarship (国家奖学金), Ministry of Education of China, 2023.
      </p>
    </li>

    <li>
      <p>
        3rd place, VisDA-2022@NeurIPS, 2022.
      </p>
    </li>

    <li>
      <p>
        Special Academic Scholarship (特等学业奖学金), Beijing Institute of Technology, 2022, 2023.
      </p>
    </li>

    <li>
      <p>
        CETC GUORUI Scholarship (国瑞奖学金), 2021.
      </p>
    </li>

    <li>
      <p>
        Excellent Undergraduate & Graduation Thesis at Beijing Institute of Technology, 2019.
      </p>
    </li>

  </ul>
</div>

<hr>
<div class="news">
  <h2>Contact</h2>
  <br />
  <ul>
    <li>
      <p>
        Email: binhuixie@bit.edu.cn
      </p>
    <li>
      <p>
        Address: Room 1106, Central Teaching Building, Beijing Institute of Technology, Beijing
      </p>
    </li>
  </ul>

</div>

<hr>